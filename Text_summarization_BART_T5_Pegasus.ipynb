{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPwEi3XfGy1XOzCtkNNEXOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mspatke/Abstractive_Summarization_BART_Transformer/blob/main/Text_summarization_BART_T5_Pegasus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl4avGKanClF"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "23FiELrtnVYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed"
      ],
      "metadata": {
        "id": "jvLadxmZoDOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\n",
        "\n",
        "print(f\"Features in cnn_dailymail : {dataset['train'].column_names}\")"
      ],
      "metadata": {
        "id": "kqdDWhPOnm1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Summarization Pipelines**"
      ],
      "metadata": {
        "id": "nhnhxHJXpa3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = dataset[\"train\"][1][\"article\"]\n",
        "\n",
        "# We'll collect the generated summaries of each model in a dictionary\n",
        "summaries = {}"
      ],
      "metadata": {
        "id": "9HJbfjSHno7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text"
      ],
      "metadata": {
        "id": "EXlmsmHPrDI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarization Baseline**"
      ],
      "metadata": {
        "id": "zBUjZ4Pjpe-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_summary(text):\n",
        "    return \"\\n\".join(sent_tokenize(text))"
      ],
      "metadata": {
        "id": "xtMGQ31_pgmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries['baseline'] = baseline_summary(sample_text)\n",
        "\n",
        "summaries['baseline']"
      ],
      "metadata": {
        "id": "jwR_H4RCpj8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**huggingface pipeline**"
      ],
      "metadata": {
        "id": "QaCOmOGfppNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "pipe = pipeline('text-generation', model = 'gpt2-medium' )\n",
        "\n",
        "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
        "\n",
        "pipe_out = pipe(gpt2_query, max_length = 512, clean_up_tokenization_spaces = True)"
      ],
      "metadata": {
        "id": "8MyS9m5ypl5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_out"
      ],
      "metadata": {
        "id": "9rHL7w35pzRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_out[0]"
      ],
      "metadata": {
        "id": "cQtmgYpmpzU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries['gpt2'] = \"\\n\".join(sent_tokenize(pipe_out[0][\"generated_text\"]))"
      ],
      "metadata": {
        "id": "VMoy3yXopzcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**T5** <br>\n",
        "T5 (Text-To-Text Transfer Transformer) is a transformer model that is trained in an end-to-end manner with text as input and modified text as output, in contrast to BERT-style models that can only output either a class label or a span of the input. This text-to-text formatting makes the T5 model fit for multiple NLP tasks like Summarization, Question-Answering, Machine Translation, and Classification problems.\n",
        "\n",
        "How T5 is different from BERT? Both T5 and BERT are trained with MLM (Masked Language Model) approach.\n",
        "\n",
        "What is MLM?\n",
        "\n",
        "The MLM is a fill-in-the-blank task, where the model masks part of the input text and tries to predict what that masked word should be.\n",
        "\n",
        "Example:\n",
        "\n",
        "“I like to eat peanut butter and\n",
        "\n",
        "The only difference is that T5 replaces multiple consecutive tokens with the single Mask Keyword, unlike, BERT which uses Mask token for each word. This illustration is shown below.\n",
        "\n",
        "T5 expects a prefix before the input text to understand the task given by the user. For example,\n",
        "“summarize:” for the summarization,\n",
        "“cola sentence:” for the classification,\n",
        "“translate English to Spanish:” for the machine translation, etc.,"
      ],
      "metadata": {
        "id": "_YlsDjy0uBSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline('summarization', model = 't5-small' )\n",
        "\n",
        "pipe_out = pipe(sample_text)"
      ],
      "metadata": {
        "id": "xWbKX-Sitp8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_out"
      ],
      "metadata": {
        "id": "rvMRpc7WuHYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries['t5'] = 'n'.join(sent_tokenize(pipe_out[0]['summary_text']))"
      ],
      "metadata": {
        "id": "0RcStobRuPQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BART**\n",
        "\n",
        "\n",
        "BART is a denoising autoencoder for pretraining sequence-to-sequence models. It is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture.\n",
        "\n",
        "That means, It uses a standard seq2seq/NMT architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT). This means the encoder's attention mask is fully visible, like BERT, and the decoder's attention mask is causal, like GPT2.\n",
        "\n",
        "This means that a fine-tuned BART model can take a text sequence (for example, English) as input and produce a different text sequence at the output (for example, French).\n",
        "\n",
        "This type of model is relevant for machine translation, question-answering , text summarization, or sequence classification (categorizing input text sentences or tokens).\n",
        "\n",
        "Another task is sentence entailment which, given two or more sentences, evaluates whether the sentences are logical extensions or are logically related to a given statement."
      ],
      "metadata": {
        "id": "R_oELLlhuWtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "pipe_out = pipe(sample_text)"
      ],
      "metadata": {
        "id": "7a6vvfxPuTyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_out"
      ],
      "metadata": {
        "id": "4-x9I2rsug5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
        "\n",
        "summaries[\"bart\"]"
      ],
      "metadata": {
        "id": "49T5sHmMuj5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PEGASUS**\n",
        "The PEGASUS model’s pre-training task is very similar to summarization, i.e. important sentences are removed and masked from an input document and are later generated together as one output sequence from the remaining sentences, which is fairly similar to a summary. In PEGASUS, several whole sentences are removed from documents during pre-training, and the model is tasked with recovering them. The Input for such pre-training is a document with missing sentences, while the output consists of the missing sentences being concatenated together. The advantage of this self-supervision is that you can create as many examples as there are documents without any human intervention, which often becomes a bottleneck problem in purely supervised systems."
      ],
      "metadata": {
        "id": "mCq6wKieu4dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline('summarization', model=\"google/pegasus-cnn_dailymail\"  )\n",
        "\n",
        "pipe_out = pipe(sample_text)"
      ],
      "metadata": {
        "id": "mx2Hn7AQuzM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_out"
      ],
      "metadata": {
        "id": "6ecgPx-Ou8LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zacAghu9u-Ho"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}